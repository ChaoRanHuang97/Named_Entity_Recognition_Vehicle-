{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61e6da9a-0d48-4a39-aee3-8ec0c7555116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and load the spacy model\n",
    "import spacy\n",
    "nlp=spacy.load(\"en_core_web_sm\") \n",
    "\n",
    "# Getting the ner component\n",
    "ner=nlp.get_pipe('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39e796e7-fc22-4280-9eb9-1f3cd91eec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json('all.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62423264-eb5d-4a5a-b17e-17ea7168bb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['data', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fca054-5d08-4190-adb5-9a3987b050ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4537db0b-2b88-4b67-9c55-84e2e197580d",
   "metadata": {},
   "source": [
    "## CASE1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "de6ab6f7-f100-4973-9d3c-54e6a19430ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "\n",
    "TRAIN_DATA = []\n",
    "fo = open('all.jsonl', \"r\")\n",
    "lines = fo.readlines()\n",
    "for line in lines:\n",
    "    line =json.loads(line)\n",
    "    if \"label\" in line:\n",
    "        line[\"entities\"] = line.pop(\"label\")\n",
    "    else:\n",
    "        line[\"entities\"] = []\n",
    "\n",
    "    tmp_ents = []\n",
    "    for e in line[\"entities\"]:\n",
    "        if e[2] in ['make', 'model', 'trim']:\n",
    "             tmp_ents.append({\"start\": e[0], \"end\": e[1], \"label\": e[2]})\n",
    "        line[\"entities\"] = tmp_ents\n",
    "    if (len(line[\"data\"]) > 5):\n",
    "        TRAIN_DATA.append(json.dumps({\"entities\": line[\"entities\"], \"text\": line[\"data\"]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b4bb15-9f0c-47cd-8f5a-0ede83d6eaac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a54e5c9b-96a7-4bb7-b682-2618a287da67",
   "metadata": {},
   "source": [
    "## CASE2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1a768be7-4ee8-4000-887b-4a91bf95ae33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "labeled_data = []\n",
    "with open(r\"all.jsonl\", \"r\") as read_file:\n",
    "    for line in read_file:\n",
    "        data = json.loads(line)\n",
    "        labeled_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9866c23e-f4f7-4aad-acfb-4016cab2c12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA = []\n",
    "for entry in labeled_data:\n",
    "    entities = []\n",
    "    for e in entry['label']:\n",
    "        entities.append((e[0], e[1],e[2]))\n",
    "    spacy_entry = (entry['data'], {\"entities\": entities})\n",
    "    TRAINING_DATA.append(spacy_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281d1e49-5d56-4fb3-952c-de5b7ea34532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9cb6f8b0-3b61-4680-8b29-0e801a16d977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "import json\n",
    "nlp = spacy.blank(\"en\")\n",
    "# ner = nlp.create_pipe(\"ner\")\n",
    "ner= nlp.add_pipe('ner')\n",
    "ner.add_label(\"make\")\n",
    "ner.add_label(\"model\")\n",
    "ner.add_label(\"trim\")\n",
    "optimizer = nlp.resume_training()\n",
    "move_names = list(ner.move_names)\n",
    "\n",
    "# List of pipes you want to train\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "\n",
    "# List of pipes which should remain unaffected in training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a32ad4eb-6e7c-4ade-bfd4-e3f75c1de173",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-8eea286b0e17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0;31m# Calling update() over the iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.35\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "from spacy.util import minibatch, compounding\n",
    "import random\n",
    "\n",
    "# Begin training by disabling other pipeline components\n",
    "with nlp.disable_pipes(*other_pipes) :\n",
    "    sizes = compounding(1.0, 4.0, 1.001)\n",
    "    # Training for 30 iterations     \n",
    "    for itn in range(30):\n",
    "        # shuffle examples before training\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        batches = minibatch(TRAIN_DATA, size=sizes)\n",
    "        # ictionary to store losses\n",
    "        losses = {}\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            # Calling update() over the iteration\n",
    "            nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "            print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049216b9-4d19-46ac-8f5a-bdb66835d524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f441d35a-ad56-4e23-868a-de674880e451",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/spacy/training/iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"For Sale 2006 Toyota Tundra SR5 V8 Cyl 4.7L Engine...\" with entities \"[(14, 20, 'make'), (20, 27, 'model'), (28, 50, 'tr...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/spacy/training/iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"2006 Chrysler 300C..HEMI..5.7L HEMI , engine ..Cre...\" with entities \"[(5, 13, 'make'), (14, 17, 'model'), (17, 18, 'tri...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/spacy/training/iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"Great vehicle for family.Very well maintained,we n...\" with entities \"[(122, 125, 'model')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 784.5292292747996}\n",
      "{'ner': 81.73998670393244}\n",
      "{'ner': 163.63509768066046}\n",
      "{'ner': 53.85248123450398}\n",
      "{'ner': 41.301099149019}\n",
      "{'ner': 31.578431538221285}\n",
      "{'ner': 31.02927501548565}\n",
      "{'ner': 23.246153879630988}\n",
      "{'ner': 311.51248985723635}\n",
      "{'ner': 21.986876120889264}\n",
      "{'ner': 21.456929596801967}\n",
      "{'ner': 14.024556979763664}\n",
      "{'ner': 261.0705402618238}\n",
      "{'ner': 11.162653316814271}\n",
      "{'ner': 8.912764354994595}\n",
      "{'ner': 82.6486727700741}\n",
      "{'ner': 92.28457846971841}\n",
      "{'ner': 15.144224153817033}\n",
      "{'ner': 172.91638540608275}\n",
      "{'ner': 6.114402283857028}\n",
      "{'ner': 71.66545406056747}\n",
      "{'ner': 10.722857371182746}\n",
      "{'ner': 28.68005498047332}\n",
      "{'ner': 6.9262309984436365}\n",
      "{'ner': 2.248697395148905}\n",
      "{'ner': 1.907833984598222}\n",
      "{'ner': 5.865848606273045}\n",
      "{'ner': 0.08808825793503126}\n",
      "{'ner': 0.0026773613635440578}\n",
      "{'ner': 1.9814335421285774}\n",
      "{'ner': 6.709283773554105e-05}\n",
      "{'ner': 30.624863298625964}\n",
      "{'ner': 2.3893173630603304}\n",
      "{'ner': 0.004014170427255903}\n",
      "{'ner': 0.07391437584235437}\n",
      "{'ner': 0.02629792134753726}\n",
      "{'ner': 1.3916758264309643}\n",
      "{'ner': 0.12172158789111055}\n",
      "{'ner': 0.3299792116095898}\n",
      "{'ner': 1.4816800282197812e-05}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import random\n",
    "import json\n",
    "from spacy.training.example import Example\n",
    "nlp = spacy.blank(\"en\")\n",
    "ner = nlp.create_pipe(\"ner\")\n",
    "nlp.add_pipe('ner')\n",
    "ner.add_label(\"make\")\n",
    "ner.add_label(\"model\")\n",
    "ner.add_label(\"trim\")\n",
    "# Start the training\n",
    "nlp.begin_training()\n",
    "# Loop for 40 iterations\n",
    "for itn in range(40):\n",
    "    # Shuffle the training data\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    losses = {}\n",
    "# Batch the examples and iterate over them\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA, size=2):\n",
    "        for text, annotations in batch:\n",
    "            doc = nlp.make_doc(text)\n",
    "            example = Example.from_dict(doc, annotations)\n",
    "            # Update the model\n",
    "            nlp.update([example], losses=losses, drop=0.3)\n",
    "    print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f879a475-2e18-4620-ae41-eb84a715a931",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "example = \"2008 Subaru Impreza coupe AWD 5SP manual 176k runs great, clean title, clean car check $4995 call 949 five four seven 6400 DLR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ef5c0565-c088-43ca-a246-eac2aa859410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">2008 \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Subaru\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">make</span>\n",
       "</mark>\n",
       " Impreza coupe AWD 5SP manual 176k runs great, clean title, clean car check $4995 call 949 five four seven 6400 DLR</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(example)\n",
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6193858d-6053-47c0-afc5-e8cc9ff3f4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.label_ + \":\", ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3db2210d-cc48-4ccc-b4ec-842ec4adb9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk('models/mca-ner-scratch.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ede146d-918c-44af-b8f6-7c139811b6a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
